{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract to hdf5: 100%|██████████| 3000/3000 [01:47<00:00, 27.92it/s] \n",
      "Extract to hdf5: 100%|██████████| 3000/3000 [01:53<00:00, 26.51it/s] \n",
      "Extract to hdf5: 100%|██████████| 3000/3000 [01:52<00:00, 26.67it/s] \n",
      "Extract to hdf5: 100%|██████████| 3000/3000 [01:53<00:00, 26.33it/s] \n"
     ]
    }
   ],
   "source": [
    "def extract_data():\n",
    "\tdis = \"dataset/raw_data_ep2\"\n",
    "\n",
    "\tos.makedirs(dis, exist_ok=True)\n",
    "\n",
    "\tfor k in range(1,5):\n",
    "\t\tf = h5py.File(os.path.join('dataset/raw_data','Part_{}.mat'.format(k)), 'r')\n",
    "\t\t\n",
    "\t\tky = 'Part_' + str(k)\n",
    "\t\t\n",
    "\t\tfor i in tqdm(range(len(f[ky])), desc=\"Extract to hdf5\"):\n",
    "\t\t\t\n",
    "\t\t\tsavHd5 = h5py.File(os.path.join(dis,'Part_{}_{}.hdf5'.format(k, i)),'w')\n",
    "\t\t\tsavHd5.create_dataset('signal', data=f[f[ky][i][0]])\n",
    "\n",
    "extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"Part_3\": shape (3000, 1), type \"|O\">\n",
      "<HDF5 object reference>\n",
      "0.5747800586510264\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "sda = h5py.File(os.path.join('dataset/raw_data','Part_3.mat'),'r')\n",
    "print(sda['Part_3'])\n",
    "print(sda['Part_3'][174][0])\n",
    "print(sda[sda['Part_3'][174][0]][1][0])\n",
    "print(len(sda['Part_3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32396481 -0.34449267 -0.37088563 -0.40118866 -0.42758162]\n",
      "[108.31679145 105.58151894]\n"
     ]
    }
   ],
   "source": [
    "sda = h5py.File('dataset/data/data_tml.hdf5', 'r')\n",
    "sda = sda['data']\n",
    "print(sda[2][0][0:5])\n",
    "print(sda[2][1][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 74.2918826   74.09650618  73.99881797  74.04766208  74.43841491\n",
      "   75.31760879  76.88062012  79.46935765  83.13266547  87.82169948\n",
      "   93.19455095  99.00699936 104.81944777 110.48536386 115.66283892\n",
      "  120.25418472 124.11286896 127.28773573 129.72994095 131.53717281\n",
      "  132.6605872  133.24671646 133.34440466 133.10018414 132.562899\n",
      "  131.73254922 130.80451124 129.72994095 128.55768244 127.28773573\n",
      "  125.87125671 124.25940127 122.3544812  120.15649651 117.61660309\n",
      "  114.73480094 111.60877826 108.43391148 105.40557702 102.67030718\n",
      "  100.42347838  98.61624653  97.1997675   96.12519721  95.34369154\n",
      "   94.8552505   94.56218587  94.31796535  94.12258893  93.87836841\n",
      "   93.63414789  93.24339506  92.80379812  92.31535707  91.77807193\n",
      "   91.24078678  90.65465753  90.06852828  89.43355492  88.74973746\n",
      "   88.06592     87.43094665  86.79597329  86.11215583  85.47718248\n",
      "   84.84220912  84.25607987  83.66995062  83.03497726  82.49769212\n",
      "   81.91156286  81.42312182  80.93468078  80.44623974  80.0066428\n",
      "   79.56704586  79.22513713  78.9320725   78.59016377  78.29709915\n",
      "   78.05287863  77.759814    77.46674938  77.17368475  76.88062012\n",
      "   76.53871139  76.24564677  75.85489394  75.56182931  75.17107648\n",
      "   74.78032364  74.48725902  74.19419439  73.85228566  73.60806514\n",
      "   73.36384462  73.1196241   72.97309178  72.87540357  72.87540357\n",
      "   73.07077999  73.51037693  74.48725902  76.24564677  78.98091661\n",
      "   82.79075674  87.52863486  93.04801864  98.90931115 104.86829187\n",
      "  110.53420797 115.66283892 120.25418472 124.11286896 127.28773573\n",
      "  129.68109684 131.4394846  132.562899   133.10018414 133.24671646\n",
      "  132.95365183 132.36752258 131.53717281 130.60913482 129.48572043\n",
      "  128.36230603 127.04351521 125.67588029 124.11286896 122.25679299\n",
      "  120.0588083  117.56775898 114.68595683 111.60877826 108.43391148\n",
      "  105.40557702 102.71915128 100.42347838  98.61624653  97.24861161\n",
      "   96.22288542  95.53906796  95.05062691  94.80640639  94.56218587\n",
      "   94.41565356  94.17143304  93.92721252  93.63414789  93.29223916\n",
      "   92.85264222  92.36420118  91.77807193  91.24078678  90.60581343\n",
      "   89.97084007  89.38471082  88.74973746  88.06592     87.43094665\n",
      "   86.79597329  86.16099994  85.57487069  84.98874143  84.35376808\n",
      "   83.71879472  83.18150958  82.69306853  82.15578339  81.66734234\n",
      "   81.2277454   80.78814847  80.44623974  80.0554869   79.81126638\n",
      "   79.46935765  79.22513713  78.98091661  78.73669609  78.54131967\n",
      "   78.29709915  78.05287863  77.85750221  77.56443758  77.32021706\n",
      "   76.97830833  76.6363996   76.29449087  75.90373804  75.61067341\n",
      "   75.31760879  74.97570006  74.68263543  74.2918826   73.99881797\n",
      "   73.65690924  73.41268872  73.1684682   73.1684682   73.36384462\n",
      "   74.09650618  75.65951752  78.24825504  81.86271876  86.55175277\n",
      "   92.07113655  97.93242907 103.94025389 109.65501409 115.07670967\n",
      "  119.81458778 123.86864843 127.23889163 129.92531736 131.87908154\n",
      "  133.24671646 134.07706623 134.41897496 134.37013085 133.97937802\n",
      "  133.44209287 132.6605872  131.73254922 130.60913482 129.38803222\n",
      "  128.0692414  126.55507417 124.84553052 122.84292225 120.54724934\n",
      "  117.90966771 115.07670967 111.9995311  108.92235253 106.04055038\n",
      "  103.54950106 101.44920457  99.73966092  98.46971421  97.49283213\n",
      "   96.80901467  96.22288542  95.78328848  95.49022385  95.09947102\n",
      "   94.80640639  94.41565356  94.07374483  93.63414789  93.19455095\n",
      "   92.6572658   92.11998066  91.4850073   90.85003395  90.21506059\n",
      "   89.62893134  88.99395798  88.31014052  87.67516717  86.99134971\n",
      "   86.30753225  85.67255889  84.98874143  84.35376808  83.71879472\n",
      "   83.18150958  82.69306853  82.10693928  81.61849824  81.1789013\n",
      "   80.73930436  80.34855153  80.0066428   79.61588996  79.32282534\n",
      "   78.98091661  78.68785198  78.34594325  78.05287863  77.759814\n",
      "   77.36906117  77.02715244  76.68524371  76.34333498  76.00142625\n",
      "   75.61067341  75.21992058  74.87801185  74.53610312  74.24303849\n",
      "   73.90112976  73.75459745  73.51037693  73.36384462  73.2173123\n",
      "   73.31500051  73.60806514  74.43841491  76.00142625  78.54131967\n",
      "   82.10693928  86.74712919  92.11998066  97.98127317 103.94025389\n",
      "  109.65501409 114.97902146 119.66805547 123.72211612 127.04351521\n",
      "  129.68109684 131.63486101 133.00249593 133.7840016  134.12591033\n",
      "  134.07706623 133.68631339 133.05134004 132.31867847 131.39064049\n",
      "  130.3160702  129.1926558  127.92270909 126.55507417 124.94321873\n",
      "  123.03829866 120.79146987 118.20273234 115.32093019 112.24375162\n",
      "  109.11772894 106.18708269 103.54950106 101.35151637  99.59312861\n",
      "   98.22549369  97.24861161  96.46710594  95.9786649   95.53906796\n",
      "   95.24600333  95.00178281  94.65987408  94.36680945  93.97605662\n",
      "   93.48761558  92.99917453  92.41304528  91.77807193  91.19194268\n",
      "   90.55696932  89.82430776  89.09164619  88.50551694  87.82169948\n",
      "   87.18672613  86.50290867  85.86793531  85.23296196  84.5979886\n",
      "   84.01185935  83.4257301   82.83960085  82.3023157   81.81387466\n",
      "   81.27658951  80.78814847  80.34855153  79.90895459  79.46935765\n",
      "   79.12744892  78.78554019  78.49247557  78.19941094  77.85750221\n",
      "   77.56443758  77.32021706  77.02715244  76.78293192  76.48986729\n",
      "   76.19680266  75.80604983  75.415297    75.07338827  74.63379133\n",
      "   74.24303849  73.85228566  73.51037693  73.1684682   72.87540357\n",
      "   72.63118305  72.38696253  72.24043022  72.09389791  72.0450538\n",
      "   72.19158611  72.63118305  73.51037693  75.17107648  77.80865811\n",
      "   81.56965413  86.30753225  91.77807193  97.73705265 103.74487747\n",
      "  109.50848178 114.68595683 119.22845853 123.13598687 126.26200954\n",
      "  128.75305886 130.60913482 131.87908154 132.6117431  132.95365183\n",
      "  132.85596362 132.51405489 131.87908154 131.04873176 130.12069378\n",
      "  129.04612349 127.87386499 126.55507417 125.13859515 123.38020739\n",
      "  121.37759912 119.03308211 116.44434459 113.51369833 110.38767566\n",
      "  107.31049709 104.47753904 101.98648972  99.93503734  98.372026\n",
      "   97.10207929  96.22288542  95.63675617  95.24600333  95.00178281\n",
      "   94.75756229  94.51334177  94.22027714  93.87836841  93.48761558\n",
      "   93.04801864  92.5595776   91.97344834  91.38731909  90.75234574\n",
      "   90.06852828  89.38471082  88.70089336  88.06592     87.38210254\n",
      "   86.74712919  86.11215583  85.42833837  84.79336502  84.25607987\n",
      "   83.66995062  83.13266547  82.54653622  82.00925107  81.47196593\n",
      "   81.03236899  80.59277205  80.15317511  79.81126638  79.42051355\n",
      "   79.12744892  78.78554019  78.49247557  78.19941094  77.90634631\n",
      "   77.66212579  77.41790527  77.12484065  76.78293192  76.48986729\n",
      "   76.19680266  75.80604983  75.415297    75.07338827  74.68263543\n",
      "   74.3407267   73.99881797  73.65690924  73.36384462  73.1196241\n",
      "   72.77771537  72.63118305  72.48465074  72.43580664  72.53349484\n",
      "   72.97309178  73.99881797  75.85489394  78.68785198  82.59538032]]\n"
     ]
    }
   ],
   "source": [
    "a = list(map(str, range(500)))\n",
    "# df = pd.DataFrame(np.array([sda[0][0]]), columns=a)\n",
    "\n",
    "print(np.array([sda[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7595307917888563\n",
      "67.0629551659967\n"
     ]
    }
   ],
   "source": [
    "sda = h5py.File(os.path.join('dataset/raw_data_ep2/','Part_1_0.hdf5'),'r')\n",
    "# print(sda['signal'])\n",
    "print(sda['signal'][0][0])\n",
    "print(sda['signal'][0][1])\n",
    "# print(sda['signal'][0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Records: 100%|██████████| 12000/12000 [54:12<00:00,  3.69it/s] \n"
     ]
    }
   ],
   "source": [
    "def process_data():\n",
    "\tpro_dir = 'dataset/processed_data'\n",
    "\traw_dir = 'dataset/raw_data_ep2'\n",
    "\n",
    "\tfs = 100\t# sampling frequency\n",
    "\tt = 10\t\t# length of ppg episodes\n",
    "\tdt = 5\t\t# step size of taking the next episode\n",
    "\n",
    "\tsamples_in_episode = round(fs * t)\t\t# number of samples in an episode\n",
    "\td_samples = round(fs * dt)\t\t\t\t# number of samples in a step\n",
    "\n",
    "\tos.makedirs(pro_dir, exist_ok=True)\n",
    "\n",
    "\traw_data = os.listdir(raw_dir)\n",
    "\n",
    "\tfor i in tqdm(range(len(raw_data)),desc='Reading Records'):\n",
    "\t\tfile = h5py.File(os.path.join(raw_dir, raw_data[i]), 'r')\n",
    "\t\tfile = file['signal']\n",
    "\n",
    "\t\tppg = []\t\t# ppg signal\n",
    "\t\tabp = []\t\t# abp signal\n",
    "\n",
    "\t\tdf = {\n",
    "\t\t\t'10s': [],\n",
    "\t\t\t'SBP': [],\n",
    "\t\t\t'DBP': []\n",
    "\t\t}\n",
    "\n",
    "\t\tfor j in range(len(file)):\n",
    "\t\t\tppg.append(file[j][0])\n",
    "\t\t\tabp.append(file[j][1])\n",
    "\n",
    "\t\tfor j in range(0, len(file) - samples_in_episode, d_samples):\n",
    "\t\t\tdf['10s'].append(j)\n",
    "\n",
    "\t\t\tsbp = max(abp[j:j+samples_in_episode])\t\t# sbp value\n",
    "\t\t\tdf['SBP'].append(sbp)\n",
    "\n",
    "\t\t\tdbp = min(abp[j:j+samples_in_episode])    \t# dbp value\n",
    "\t\t\tdf['DBP'].append(dbp)\n",
    "\t\t\n",
    "\t\tpd.DataFrame(df).to_csv(\n",
    "\t\t\tos.path.join(pro_dir, f'{raw_data[i].split(\".\")[0]}.csv'),\n",
    "\t\t\tindex = False\n",
    "\t\t)\n",
    "\n",
    "process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DBP Binning: 100%|██████████| 120/120 [00:00<00:00, 167.32it/s]\n",
      "SBP Binning: 100%|██████████| 131/131 [00:03<00:00, 34.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def downsample_data(minThresh=2500, ratio=0.25):\n",
    "    files = next(os.walk('dataset/processed_data'))[2]\n",
    "    sbps_dict = {}      # dictionary to store sbp and dbp values\n",
    "    dbps_dict = {}\n",
    "    \n",
    "    sbps_cnt = {}       # dictionary containing count of specific sbp and dbp values\n",
    "    dbps_cnt = {}\n",
    "    \n",
    "    dbps_taken = {}     # dictionary containing count of specific sbp and dbp taken\n",
    "    sbps_taken = {}\n",
    "    \n",
    "    candidates = []\t\t# list of candidate episodes\n",
    "    \n",
    "    lut = {}            # look up table\n",
    "\t\n",
    "    for fl in files:\n",
    "        lines = pd.read_csv(os.path.join('dataset/processed_data', fl))\n",
    "        lines = lines.values\n",
    "\n",
    "        for line in lines:\n",
    "            file_no = int(fl.split('_')[1])\n",
    "            record_no = int(fl.split('.')[0].split('_')[2])\n",
    "            episode_st = int(line[0])\n",
    "            sbp = int(float(line[1]))\n",
    "            dbp = int(float(line[2]))\n",
    "\n",
    "            if sbp not in sbps_dict:\n",
    "                sbps_dict[sbp] = []\n",
    "                sbps_cnt[sbp] = 0\n",
    "            \n",
    "            sbps_dict[sbp].append((file_no, record_no, episode_st))\n",
    "            sbps_cnt[sbp] += 1\n",
    "\n",
    "            if dbp not in dbps_dict:\n",
    "                dbps_dict[dbp] = []\n",
    "                dbps_cnt[dbp] = 0\n",
    "            \n",
    "            dbps_dict[dbp].append((file_no, record_no, episode_st, sbp))\n",
    "            dbps_cnt[dbp] += 1\n",
    "    \n",
    "    sbp_keys = list(sbps_dict)\t\t\t\t# all the different sbp values\n",
    "    dbp_keys = list(dbps_dict)\t\t\t\t# all the different dbp values\n",
    "    \n",
    "    sbp_keys.sort()\t\t\t\t\t# sorting the sbp values\n",
    "    dbp_keys.sort()\t\t\t\t\t# sorting the dbp values\n",
    "\n",
    "    for dbp in tqdm(dbp_keys, desc='DBP Binning'):\n",
    "        cnt = min(int(dbps_cnt[dbp] * ratio), minThresh)\n",
    "\n",
    "        for i in range(cnt):\n",
    "            indix = np.random.randint(len(dbps_dict[dbp]))\n",
    "            candidates.append([dbps_dict[dbp][indix][0], dbps_dict[dbp][indix][1], dbps_dict[dbp][indix][2]])\n",
    "\n",
    "            if(dbp not in dbps_taken):\n",
    "                dbps_taken[dbp] = 0\n",
    "\n",
    "            dbps_taken[dbp] += 1\n",
    "            \n",
    "            if(dbps_dict[dbp][indix][3] not in sbps_taken):\n",
    "                sbps_taken[dbps_dict[dbp][indix][3]] = 0\n",
    "            \n",
    "            sbps_taken[dbps_dict[dbp][indix][3]] += 1\n",
    "\n",
    "            if(dbps_dict[dbp][indix][0] not in lut):\n",
    "                lut[dbps_dict[dbp][indix][0]] = {}\n",
    "            \n",
    "            if(dbps_dict[dbp][indix][1] not in lut[dbps_dict[dbp][indix][0]]):\n",
    "                lut[dbps_dict[dbp][indix][0]][dbps_dict[dbp][indix][1]] = {}\n",
    "            \n",
    "            if(dbps_dict[dbp][indix][2] not in lut[dbps_dict[dbp][indix][0]][dbps_dict[dbp][indix][1]]):\n",
    "                lut[dbps_dict[dbp][indix][0]][dbps_dict[dbp][indix][1]][dbps_dict[dbp][indix][2]] = 1\n",
    "            \n",
    "            dbps_dict[dbp].pop(indix)\n",
    "\n",
    "    for sbp in tqdm(sbp_keys, desc='SBP Binning'):\n",
    "        if sbp not in sbps_taken:\n",
    "            sbps_taken[sbp] = 0\n",
    "        \n",
    "        cnt = min(int(sbps_cnt[sbp]*ratio), minThresh) - sbps_taken[sbp]\n",
    "        \n",
    "        for i in range(cnt):\n",
    "            while len(sbps_dict[sbp]) > 0:\n",
    "                try:\n",
    "                    indix = np.random.randint(len(sbps_dict[sbp]))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    dumi = lut[sbps_dict[sbp][indix][0]][sbps_dict[sbp][indix][1]][sbps_dict[sbp][indix][2]]\n",
    "                except:\n",
    "                    sbps_dict[sbp].pop(indix)\n",
    "                    continue\n",
    "\n",
    "                candidates.append([sbps_dict[sbp][indix][0], sbps_dict[sbp][indix][1], sbps_dict[sbp][indix][2]])\n",
    "                sbps_taken[sbp] += 1\n",
    "                sbps_dict[sbp].pop(indix)\n",
    "                break\n",
    "            \n",
    "    pickle.dump(candidates, open('dataset/candidates.p', 'wb'))\n",
    "\n",
    "downsample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracts the episodes: 100%|██████████| 149047/149047 [07:45<00:00, 319.84it/s]\n",
      "Extracts the episodes: 100%|██████████| 149047/149047 [09:22<00:00, 264.75it/s]\n",
      "Extracts the episodes: 100%|██████████| 149047/149047 [05:36<00:00, 442.60it/s] \n",
      "Extracts the episodes: 100%|██████████| 149047/149047 [09:02<00:00, 274.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_episodes(candidates):\n",
    "    os.makedirs('dataset/ppgs', exist_ok=True)\n",
    "    os.makedirs('dataset/abps', exist_ok=True)\n",
    "\n",
    "    fs = 100\t# sampling frequency\n",
    "    t = 10      # length of ppg episodes\t\t\t\n",
    "    samples_in_episode = round(fs * t)\n",
    "\n",
    "    for k in range(1, 5):\n",
    "        for indix in tqdm(range(len(candidates)), desc='Extracts the episodes'):\n",
    "            if candidates[indix][0] != k:\n",
    "                continue\n",
    "\n",
    "            record_no = int(candidates[indix][1])\n",
    "            episode_st = int(candidates[indix][2])\n",
    "\n",
    "            file = h5py.File(os.path.join('dataset/raw_data_ep2', f'Part_{k}_{record_no}.hdf5'), 'r')\n",
    "            file = file['signal']\n",
    "\n",
    "            p_ppg = []\n",
    "            ppg = []\n",
    "            abp = []\n",
    "\n",
    "            for j in range(episode_st, episode_st+samples_in_episode):\n",
    "                p_ppg.append(file[j][0])\n",
    "                abp.append(file[j][1])\n",
    "            \n",
    "            mean = sum(p_ppg)/samples_in_episode\n",
    "            ppg = [x - mean for x in p_ppg]\n",
    "\n",
    "            pickle.dump(np.array(ppg), open(os.path.join('dataset/ppgs', '{}.p'.format(indix)), 'wb'))\n",
    "            pickle.dump(np.array(abp), open(os.path.join('dataset/abps', '{}.p'.format(indix)), 'wb'))\n",
    "\n",
    "candidates = pickle.load(open('dataset/candidates.p', 'rb'))\n",
    "extract_episodes(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149047/149047 [02:26<00:00, 1019.11it/s]\n"
     ]
    }
   ],
   "source": [
    "def merge_episodes():\n",
    "    os.makedirs('dataset/data', exist_ok=True)\n",
    "\n",
    "    files = next(os.walk('dataset/abps'))[2]\n",
    "    \n",
    "    np.random.shuffle(files)\n",
    "    data = []\n",
    "\n",
    "    for fl in tqdm(files):\n",
    "        ppg = pickle.load(open(os.path.join('dataset/ppgs', fl), 'rb'))\n",
    "        abp = pickle.load(open(os.path.join('dataset/abps', fl), 'rb'))\n",
    "        \n",
    "        data.append([ppg, abp])\n",
    "    \n",
    "    f = h5py.File(os.path.join('dataset/data','data.hdf5'), 'w')\n",
    "    f.create_dataset('data', data=data)\n",
    "\n",
    "merge_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Data : 100%|██████████| 100000/100000 [01:15<00:00, 1324.39it/s]\n",
      "Validation Data : 100%|██████████| 40000/40000 [00:29<00:00, 1358.12it/s]\n",
      "Test Data: 100%|██████████| 9047/9047 [00:06<00:00, 1311.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def split_data():\n",
    "    sst = '_1'\n",
    "    length = 500\n",
    "\n",
    "    fl = h5py.File(os.path.join('dataset/data', 'data.hdf5'), 'r')      # load the episode data\n",
    "\n",
    "    X_train = []                        # intialize train data\n",
    "    Y_train = []\n",
    "\n",
    "    X_val = []                          # intialize validation data\n",
    "    Y_val = []\n",
    "\n",
    "    for i in tqdm(range(0, 100000), desc=f'Training Data '):    # training samples before validation samples\n",
    "\n",
    "        X_train.append(np.array(fl['data'][i][0][:length]).reshape(-1, 1))  # ppg signal\n",
    "\n",
    "        sbp = max(fl['data'][i][1][:length])       # update min-max of abp\n",
    "        dbp = min(fl['data'][i][1][:length])\n",
    "\n",
    "        Y_train.append(np.array([sbp, dbp]).reshape(-1, 1))  # abp signal\n",
    "\n",
    "    for i in tqdm(range(100000, 140000), desc=f'Validation Data '):\n",
    "\n",
    "        X_val.append(np.array(fl['data'][i][0][:length]).reshape(-1, 1))  # ppg signal\n",
    "\n",
    "        sbp = max(fl['data'][i][1][:length])       # update min-max of abp\n",
    "        dbp = min(fl['data'][i][1][:length])\n",
    "\n",
    "        Y_val.append(np.array([sbp, dbp]).reshape(-1, 1))  # abp signal\n",
    "\n",
    "    X_train = np.array(X_train)             # converting to numpy array\n",
    "\n",
    "    Y_train = np.array(Y_train)             # converting to numpy array\n",
    "    \n",
    "    # saving the training data split\n",
    "    pickle.dump({'X_train': X_train, 'Y_train': Y_train}, open(os.path.join('dataset/data', f'train{sst}.p'), 'wb'))\n",
    "\n",
    "    X_val = np.array(X_val)                 # converting to numpy array        \n",
    "\n",
    "    Y_val = np.array(Y_val)                 # converting to numpy array\n",
    "    \n",
    "    # saving the validation data split\n",
    "    pickle.dump({'X_val': X_val, 'Y_val': Y_val}, open(os.path.join('dataset/data', f'val{sst}.p'), 'wb'))\n",
    "\n",
    "    X_test = []                 # intialize test data\n",
    "    Y_test = []\n",
    "\n",
    "    for i in tqdm(range(140000, len(fl['data'])), desc='Test Data'):\n",
    "\n",
    "        X_test.append(np.array(fl['data'][i][0][:length]).reshape(-1, 1))       # ppg signal\n",
    "\n",
    "        sbp = max(fl['data'][i][1][:length])\n",
    "        dbp = min(fl['data'][i][1][:length])\n",
    "        Y_test.append(np.array([sbp, dbp]).reshape(-1, 1))       # abp signal\n",
    "\n",
    "    X_test = np.array(X_test)           # converting to numpy array\n",
    "    \n",
    "    Y_test = np.array(Y_test)           # converting to numpy array\n",
    "\n",
    "                                                                # saving the test data split\n",
    "    pickle.dump({'X_test': X_test,'Y_test': Y_test}, open(os.path.join('dataset/data', f'test{sst}.p'), 'wb'))\n",
    "\n",
    "split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# rm -rf dataset/raw_data_ep2/*\n",
    "# rm -rf dataset/processed_data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "ls dataset/raw_data_ep2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
